\documentclass[a4paper,twoside, 10pt]{article}
\usepackage[%CJKbookmarks=true,
	unicode=true,
	hyperindex=true,
	pdfstartview=FitH,
	bookmarksnumbered=true,	%注释掉此行则书签前没有数字编号
	bookmarksopen=true,  	%注释掉此行则默认不展开书签
	colorlinks=true, 	%注释掉此项则交叉引用为彩色边框(将colorlinks和pdfborder同时注释掉)
	pdfborder=001,   	%注释掉此项则交叉引用为彩色边框
	citecolor=red
]{hyperref}

\usepackage{graphicx}		%图像宏包
\usepackage{amsmath}		%ams数学宏包
\usepackage{algorithm}		% algorithm package
\usepackage{algpseudocode}	% algorithmicx package is included therein
\usepackage{simplewick}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsthm}
\DeclareMathOperator{\Tr}{Tr}

\newtheorem{theorem}{Theorem}
\theoremstyle{wick}
\newtheorem*{wick}{Wick's Theorem}

\newtheorem{lemma}[theorem]{Lemma}

\usepackage{extarrows}		%长等号、等号上下
\usepackage{textcomp}		%摄氏度
\usepackage{multirow,makecell}
\usepackage{mhchem}
\usepackage{pifont}
\usepackage{subfig}		%subfloat

\newcommand{\mr}{\mathrm}
\newcommand{\tr}{\textrm}
\newcommand{\tN}{\tr{N}}
\newcommand{\md}{\mathrm{d}}
\newcommand{\mi}{\mathrm{i}}
\newcommand{\me}{\mathrm{e}}
\newcommand{\mg}{\mathrm{g}}
\newcommand{\br}{\bm{r}}
\newcommand{\bR}{\bm{R}}
\newcommand{\bt}{\bm{\tau}}
\newcommand{\bk}{\bm{k}}
\newcommand{\bK}{\bm{K}}
\newcommand{\ba}{\bm{a}}
\newcommand{\bb}{\bm{b}}
\newcommand{\bG}{\bm{G}}
\newcommand{\bq}{\bm{q}}
\newcommand{\bB}{\bm{B}}
\newcommand{\bx}{\bm{x}}
\newcommand{\vD}{\varDelta}
\newcommand{\Ao}{\mathring{\mr{A}}}
\newcommand{\tc}{\textcelsius}
\newcommand{\oo}{$\ddot{\mathrm{o}}$}
\newcommand{\hh}{\hat{E}}
\newcommand{\mcH}{\mathcal{H}}
\newcommand{\mcS}{\mathcal{S}}
\newcommand{\mcM}{\mathcal{M}}
\newcommand{\psik}{\psi_{\bm{k}}(\bm{r})}
\newcommand{\vpr}{V^{\mr{psd}}(\bm{r})}
\newcommand{\vpq}{V^{\mr{psd}}(q)}
\newcommand{\rlh}{\rightleftharpoons}
\newcommand{\II}{\tr{\textit{II}}}
\newcommand{\IV}{\tr{\textit{IV}}}
\newcommand{\oscf}{$\varOmega$-SCF}
\newcommand{\sscf}{$\sigma$-SCF}
\newcommand{\Oo}{\varOmega(\omega)}
\newcommand{\mcF}{\mathcal{F}}

\newcommand{\sff}{\sffamily}

\newcommand{\cd}{c^{\dagger}}
\newcommand{\ad}{a^{\dagger}}
\newcommand{\exphf}[1]{\langle{}#1\rangle_0}
\newcommand{\mn}{ij\lambda\sigma}
\newcommand{\mnp}{i'j'\lambda'\sigma'}
\newcommand{\ti}{\langleij|\lambda\sigma\rangle}
\newcommand{\tiv}{\langleij|\sigma\lambda\rangle}
\newcommand{\tip}{\langlei'j'|\lambda'\sigma'\rangle}
\newcommand{\tipv}{\langlei'j'|\sigma'\lambda'\rangle}
\newcommand{\ex}[1]{\langle{}#1\rangle}
\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\bra}[1]{\langle{}#1|}
\newcommand{\var}{\sigma_H^2}
\newcommand{\Ns}{N_{s}}
\newcommand{\Nt}{N_{t}}
\newcommand{\sums}[1]{\sum_{#1}^{s}}
\newcommand{\sumt}[1]{\sum_{#1}^{t}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\mmp}{ii'}
\newcommand{\nnp}{jj'}
\newcommand{\llp}{\lambda\lambda'}
\newcommand{\ssp}{\sigma\sigma'}
\newcommand{\pfrac}[2]{\frac{\partial{}#1}{\partial{}#2}}

\usepackage{amssymb}
\usepackage{color}
\usepackage{xcolor}		%颜色
\usepackage{bm}			%数学矢量宏包
\usepackage{mathrsfs}
\pagestyle{plain}
\usepackage[raggedright]{titlesec}
\usepackage[procnames]{listings}

\usepackage{pict2e}
\usepackage{keyval}
%\usepackage{fp}
\usepackage{diagbox}		%这几个宏包用于画表头的斜线
\usepackage{booktabs}		%三种粗细不同的线：\toprule、\midrule 和 \bottomrule 对应顶部、中部、底部
\usepackage[font=small,labelfont=bf,width=1.0\textwidth]{caption}
\captionsetup{tablename=Tab. }
\captionsetup{figurename=Fig.}


\usepackage[font=small,labelfont=bf,width=1.0\textwidth]{caption}
\usepackage{multirow}
\usepackage{cases}

\usepackage{appendix}		%附录

\bibliographystyle{unsrt}
\newcommand{\upcite}[1]{\textsuperscript{\cite{#1}}}	%upcite命令

\usepackage{geometry}
\geometry{left=2.7cm,right=2.6cm,top=2.5cm,bottom=2.5cm}	%页边距

%%\linespread{1.2}				%行距

\title{Basic Exercises for Scientific Programming}
\author{Hongzhou Ye}
\date{January 1, 2018}							%开启我则不显示日期

\newcommand{\ttf}{\ttfamily}
\newcommand{\ttt}{\texttt}

\begin{document}
	\maketitle{}

	\begin{abstract}
		Scientific programming involves many topics of applied math, varying from basic linear algebra to optimization algorithms. This file is a collection of several basic exercises that help you practice some of the essential skills.
	\end{abstract}

	\section{Linear least square fitting}

	In the most general sense, the task of least square fitting is to find an approximate mapping $\tilde{f}$ to a given mapping
	\begin{equation}
	\begin{split}
		f:&\, \mathbb{X} \rightarrow \mathbb{Y}	\\
		  &\, \bm{x} \rightarrow \bm{y} = f(\bm{x})
	\end{split}
	\end{equation}
	by minimizing the least square loss
	\begin{equation}
		\mathcal{L}
			= \sum_{i} \big\|\tilde{f}(\bm{x}^{(i)}) - \bm{y}^{(i)}\big\|_2^2
	\end{equation}
	based on a set of known data $\{(\bm{x}^{(i)}, \bm{y}^{(i)})\}$, where
	\begin{equation}
		\|\bm{a}\|_2
			= \sqrt{\bm{a} \cdot \bm{a}}
			= \bigg(\sum_{\mu} a_{\mu}^2\bigg)^{1/2}
	\end{equation}
	is the $2$-norm of vector $\bm{a}$.

	As a special case, linear least square fitting assumes a linear functional form for the approximate mapping $\tilde{f}$, i.e.
	\begin{equation}	\label{eq:lls_ansatz}
		\tilde{f}(\bm{x})
			= \mat{A}^{\tr{T}}\bm{x} + \bm{b},
	\end{equation}
	where $\mat{A}$ and $\bm{b}$ are determined from experimental data. In this exercise, we are going to explore the properties of linear least square fitting.

	\begin{enumerate}
		\item First let us consider an even simpler case, $\bm{b} = \bm{0}$. Show that $\mat{A}$ is determined by the following equation
		\begin{equation}	\label{eq:lls_equation}
			\mat{X} \mat{X}^{\tr{T}} \mat{A}
				= \mat{X} \mat{Y}^{\tr{T}}
		\end{equation}
		where
		\begin{equation}	\label{eq:data_set_def}
			\mat{X}
				= [\bm{x}^{(1)}, \bm{x}^{(2)}, \cdots{}, \bm{x}^{(N)}],\quad{}
			\mat{Y}
				= [\bm{y}^{(1)}, \bm{y}^{(2)}, \cdots{}, \bm{y}^{(N)}].
		\end{equation}

		\textit{Hint}: $\mathcal{L}$ is a function of $\mat{A}$
		\begin{equation}	\label{eq:lsloss}
		\begin{split}
			\mathcal{L}
				&= \sum_i \| \mat{A}^{\tr{T}} \bm{x}^{(i)} - \bm{y}^{(i)} \|^2_2
				= \sum_i (\mat{A}^{\tr{T}} \bm{x}^{(i)} - \bm{y}^{(i)})^{\tr{T}}
				(\mat{A}^{\tr{T}} \bm{x}^{(i)} - \bm{y}^{(i)})	\\
				\iffalse
				&= \mat{A}^{\tr{T}} \bigg[\sum_{i} \bm{x}^{(i)} \bm{x}^{(i)\tr{T}}\bigg] \mat{A}
				- \mat{A}^{\tr{T}} \bigg[\sum_{i} \bm{x}^{(i)} \bm{y}^{(i)\tr{T}}\bigg]
				- \bigg[\sum_{i} \bm{y}^{(i)} \bm{x}^{(i)\tr{T}} \bigg] \mat{A}
				+ \sum_i \bm{y}^{(i)\tr{T}} \bm{y}^{(i)}
				\fi
		\end{split}
		\end{equation}
		You might need the following trick
		\begin{equation}
		\begin{split}
			\sum_i \bm{x}^{(i)\tr{T}} \mat{A} \mat{A}^{\tr{T}} \bm{x}^{(i)}
				&= \sum_i \sum_{\mu\nu\lambda} x^{(i)}_{\mu} A_{\mu\nu}
				A^{\tr{T}}_{\nu\lambda} x^{(i)}_{\lambda}
				= \sum_{\mu\nu\lambda} A^{\tr{T}}_{\nu\lambda} \bigg[\sum_i x^{(i)}_{\lambda}
				x^{(i)}_{\mu}\bigg] A_{\mu\nu}	\\
				&= \sum_{\mu\nu\lambda} A^{\tr{T}}_{\nu\lambda} (XX^{\tr{T}})_{\lambda \mu} A_{\mu\nu}
				= \mat{A}^{\tr{T}} \mat{X} \mat{X}^{\tr{T}} \mat{A}
		\end{split}
		\end{equation}
		Same trick can be played to all other terms in the expansion. Then you can take derivative with $\mat{A}^{\tr{T}}$ and obtain the desired equation (you can treat $\mat{A}$ and $\mat{A}^{\tr{T}}$ two independent variables).

		\item We now have the equation and let us see how it works! In \ttt{data/least\_square} you can find the data file \ttt{X1.txt} and \ttt{Y1.txt}, which contains $20$ data points. Fit a linear equation with zero $\bm{b}$ and plot the fitted curve along with the original data. Your result should be something like the following
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.6\linewidth]{plot/least_square/XY1.png}
		\end{figure}

		\item The example above is one-dimensional and trivial. A non-trivial multi-dimensional data set can be found in \ttt{X2.txt} and \ttt{Y2.txt}, where $\bm{x}^{(i)} \in \mathbb{R}^{5}$ and $\bm{y}^{(i)} \in \mathbb{R}^3$ and there are $20$ of them. Find an $\mat{A} \in \mathbb{R}^{5 \times{} 3}$ that minimizes the square loss. If you do the math correctly, the least square loss as defined in eqn (\ref{eq:lsloss}) should be $3.431402129330746$.
	\end{enumerate}

	\section{Non-linear fitting in terms of LLS}

	The reason why linear least square (LLS) is important is that one can go beyond linear fitting by introducing basis functions.

	To be specific, let us first specify the dimension for each quantity:
	\begin{equation}
		\bm{x} \in \mathbb{R}^n,\quad{}
		\bm{y} \in \mathbb{R}^m,\quad{}
		\tr{data set: }\{(\bm{x}^{(i)}, \bm{y}^{(i)})\}_{i = 1}^N.
	\end{equation}
	Now we introduce a basis function:
	\begin{equation}
	\begin{split}
		\phi:&\, \mathbb{R}^n \to \mathbb{R}^l		\\
			 &\, \bm{x} \to \bm{\phi} = \phi(\bm{x})
	\end{split}
	\end{equation}
	For example, $\phi(x) = [x_1^2, x_2^2, x_1x_2]^{\tr{T}}$ is a basis function transforming a $\mathbb{R}^2$ vector $\bm{x} = [x_1, x_2]^{\tr{T}}$ to a $\mathbb{R}^3$ vector $\bm{\phi}$. With this in hands, our ansatz in eqn (\ref{eq:lls_ansatz}) needs to be modified
	\begin{equation}	\label{eq:lls_ansatz_with_basis}
		\tilde{f}(\bm{x})
			= \mat{A}^{\tr{T}} \phi(\bm{x}) + \bm{b}
	\end{equation}
	and $\mat{A} \in \mathbb{R}^{l \times{} m}$.

	Though eqn (\ref{eq:lls_ansatz_with_basis}) is still linear in $\bm{\phi}$, it does not need to be linear in $\bm{x}$ because the basis function could be non-linear. In this way, one can achieve non-linearity within the framework of LLS.

	\begin{enumerate}
		\item In a way similar to the derivation of eqn (\ref{eq:lls_equation}), show that $\mat{A}$ in eqn (\ref{eq:lls_ansatz_with_basis}) is determined by solving
		\begin{equation}
			\mat{\Phi} \mat{\Phi}^{\tr{T}} \mat{A}
				= \mat{\Phi} \mat{Y}^{\tr{T}}
		\end{equation}
		where $\mat{Y}$ is defined in eqn (\ref{eq:data_set_def}), and
		\begin{equation}
			\mat{\Phi}
				= [\bm{\phi}^{(1)}, \bm{\phi}^{(2)}, \cdots{}, \bm{\phi}^{(N)}].
		\end{equation}

		\item \textit{Polynomial basis}. LLS with basis function
		\begin{equation}
			\phi(x)
				= [1, x, x^2, \cdots{}, x^{l-1}]
		\end{equation}
		is equivalent to fitting a polynomial of order $l-1$. Perform LLS fit with $l = 2, 4, 6$ and $8$ for data sets \ttt{X1.txt} and \ttt{Y1.txt} in \ttt{data/non\_linear\_least\_square}. Then
		\begin{enumerate}
			\item report the loss for each $l$ and
			\item plot the fitted curves along with the data points
		\end{enumerate}
		What trend(s) do you observe? [\textit{Hint}: wrap the fitting process in a function that takes two matrices (in this special case, vectors) $\mat{X}$ and $\mat{Y}$ and the fitting order $l$ as input parameters.]

		\item \textit{Polynomial basis (cnt'd)}. The data set above is generated as follows
		\begin{equation}
			y
				= 1 - 2x + x^3 + g
		\end{equation}
		where $g \sim \mathcal{N}(0, 1)$ is a Gaussian random variable that mimics a random noise (from, e.g.\ measure error). Now plot your fitted curves and the original data points along with the ``real" solution (set $g = 0$).
		\begin{enumerate}
			\item Does it approach the ``real" solution for larger $l$?
			\item Does a small loss always mean a \emph{good} fitting?
		\end{enumerate}

		\item \textit{Polynomial basis (cnt'd)}. By using more data points, we can have a more confident fitting. Data sets \ttt{X2.txt} and \ttt{Y2.txt} are generated in the same way, but of a much larger size! Now re-do all fittings with the new data and plot the fitted curves along with the ``real" solution, what new trend(s) do you observe?
	\end{enumerate}

\end{document}
